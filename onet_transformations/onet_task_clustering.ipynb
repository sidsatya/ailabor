{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24878ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx   # library for creating graphs/networks\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "# Ensure the OpenAI API key is set in the environment\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_openai_embeddings(texts, model='text-embedding-ada-002'): \n",
    "    # texts: list of strings\n",
    "    response = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=model\n",
    "    )\n",
    "    # Return a list of embeddings\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "def create_embeddings(data, batch_size=512, save_path=None):\n",
    "    embeddings = []\n",
    "    for i in range (0, len(data), batch_size): \n",
    "        batch = data['task_clean'].iloc[i:i+batch_size].tolist()\n",
    "        print(f\"Processing batch from index {i} to {min(i + batch_size, len(data)) - 1}\")\n",
    "        batch_embeddings = get_openai_embeddings(batch)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        print(f\"Processed batch {i // batch_size + 1}/{(len(data) + batch_size - 1) // batch_size}\")\n",
    "\n",
    "    # format of return is [[embedding1], [embedding2], ...]\n",
    "    E = np.vstack(embeddings)\n",
    "\n",
    "    if save_path:\n",
    "        np.save(save_path, E)\n",
    "        print(f\"Embeddings saved to {save_path}\")\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bc2931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch from index 0 to 511\n",
      "Processed batch 1/64\n",
      "Processing batch from index 512 to 1023\n",
      "Processed batch 2/64\n",
      "Processing batch from index 1024 to 1535\n",
      "Processed batch 3/64\n",
      "Processing batch from index 1536 to 2047\n",
      "Processed batch 4/64\n",
      "Processing batch from index 2048 to 2559\n",
      "Processed batch 5/64\n",
      "Processing batch from index 2560 to 3071\n",
      "Processed batch 6/64\n",
      "Processing batch from index 3072 to 3583\n",
      "Processed batch 7/64\n",
      "Processing batch from index 3584 to 4095\n",
      "Processed batch 8/64\n",
      "Processing batch from index 4096 to 4607\n",
      "Processed batch 9/64\n",
      "Processing batch from index 4608 to 5119\n",
      "Processed batch 10/64\n",
      "Processing batch from index 5120 to 5631\n",
      "Processed batch 11/64\n",
      "Processing batch from index 5632 to 6143\n",
      "Processed batch 12/64\n",
      "Processing batch from index 6144 to 6655\n",
      "Processed batch 13/64\n",
      "Processing batch from index 6656 to 7167\n",
      "Processed batch 14/64\n",
      "Processing batch from index 7168 to 7679\n",
      "Processed batch 15/64\n",
      "Processing batch from index 7680 to 8191\n",
      "Processed batch 16/64\n",
      "Processing batch from index 8192 to 8703\n",
      "Processed batch 17/64\n",
      "Processing batch from index 8704 to 9215\n",
      "Processed batch 18/64\n",
      "Processing batch from index 9216 to 9727\n",
      "Processed batch 19/64\n",
      "Processing batch from index 9728 to 10239\n",
      "Processed batch 20/64\n",
      "Processing batch from index 10240 to 10751\n",
      "Processed batch 21/64\n",
      "Processing batch from index 10752 to 11263\n",
      "Processed batch 22/64\n",
      "Processing batch from index 11264 to 11775\n",
      "Processed batch 23/64\n",
      "Processing batch from index 11776 to 12287\n",
      "Processed batch 24/64\n",
      "Processing batch from index 12288 to 12799\n",
      "Processed batch 25/64\n",
      "Processing batch from index 12800 to 13311\n",
      "Processed batch 26/64\n",
      "Processing batch from index 13312 to 13823\n",
      "Processed batch 27/64\n",
      "Processing batch from index 13824 to 14335\n",
      "Processed batch 28/64\n",
      "Processing batch from index 14336 to 14847\n",
      "Processed batch 29/64\n",
      "Processing batch from index 14848 to 15359\n",
      "Processed batch 30/64\n",
      "Processing batch from index 15360 to 15871\n",
      "Processed batch 31/64\n",
      "Processing batch from index 15872 to 16383\n",
      "Processed batch 32/64\n",
      "Processing batch from index 16384 to 16895\n",
      "Processed batch 33/64\n",
      "Processing batch from index 16896 to 17407\n",
      "Processed batch 34/64\n",
      "Processing batch from index 17408 to 17919\n",
      "Processed batch 35/64\n",
      "Processing batch from index 17920 to 18431\n",
      "Processed batch 36/64\n",
      "Processing batch from index 18432 to 18943\n",
      "Processed batch 37/64\n",
      "Processing batch from index 18944 to 19455\n",
      "Processed batch 38/64\n",
      "Processing batch from index 19456 to 19967\n",
      "Processed batch 39/64\n",
      "Processing batch from index 19968 to 20479\n",
      "Processed batch 40/64\n",
      "Processing batch from index 20480 to 20991\n",
      "Processed batch 41/64\n",
      "Processing batch from index 20992 to 21503\n",
      "Processed batch 42/64\n",
      "Processing batch from index 21504 to 22015\n",
      "Processed batch 43/64\n",
      "Processing batch from index 22016 to 22527\n",
      "Processed batch 44/64\n",
      "Processing batch from index 22528 to 23039\n",
      "Processed batch 45/64\n",
      "Processing batch from index 23040 to 23551\n",
      "Processed batch 46/64\n",
      "Processing batch from index 23552 to 24063\n",
      "Processed batch 47/64\n",
      "Processing batch from index 24064 to 24575\n",
      "Processed batch 48/64\n",
      "Processing batch from index 24576 to 25087\n",
      "Processed batch 49/64\n",
      "Processing batch from index 25088 to 25599\n",
      "Processed batch 50/64\n",
      "Processing batch from index 25600 to 26111\n",
      "Processed batch 51/64\n",
      "Processing batch from index 26112 to 26623\n",
      "Processed batch 52/64\n",
      "Processing batch from index 26624 to 27135\n",
      "Processed batch 53/64\n",
      "Processing batch from index 27136 to 27647\n",
      "Processed batch 54/64\n",
      "Processing batch from index 27648 to 28159\n",
      "Processed batch 55/64\n",
      "Processing batch from index 28160 to 28671\n",
      "Processed batch 56/64\n",
      "Processing batch from index 28672 to 29183\n",
      "Processed batch 57/64\n",
      "Processing batch from index 29184 to 29695\n",
      "Processed batch 58/64\n",
      "Processing batch from index 29696 to 30207\n",
      "Processed batch 59/64\n",
      "Processing batch from index 30208 to 30719\n",
      "Processed batch 60/64\n",
      "Processing batch from index 30720 to 31231\n",
      "Processed batch 61/64\n",
      "Processing batch from index 31232 to 31743\n",
      "Processed batch 62/64\n",
      "Processing batch from index 31744 to 32255\n",
      "Processed batch 63/64\n",
      "Processing batch from index 32256 to 32679\n",
      "Processed batch 64/64\n",
      "Embeddings saved to ../data/onet/task_embeddings.npy\n",
      "Embeddings created successfully. Shape of embeddings: (32680, 1536)\n"
     ]
    }
   ],
   "source": [
    "# read the data from the csv file\n",
    "data_path = os.path.join('../data/onet/task_counts.csv')\n",
    "\n",
    "data = pd.read_csv(data_path, encoding='latin1')\n",
    "\n",
    "# clean the data by lowering the case and removing extra spaces. The regex `r'\\s+'` matches one or more whitespace characters.\n",
    "data['task_clean'] = data['Task'].str.lower().replace(r'\\s+', ' ', regex=True).str.strip()  # remove extra spaces\n",
    "\n",
    "'''\n",
    "Step 1:\n",
    "Get the embedding for each task statement using OpenAI's text-embedding-ada-002 model.\n",
    "'''\n",
    "E = create_embeddings(data, batch_size=512, save_path='../data/onet/task_embeddings.npy')\n",
    "\n",
    "print(\"Embeddings created successfully. Shape of embeddings:\", E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 2: \n",
    "Once we have embeddings, we can first normalize each embedding to have an L2 norm of 1 (i.e., make them unit vectors), \n",
    "and then we can use FAISS to find the top-k (k=50) nearest neighbors for each embedding based on cosine similarity. \n",
    "We want to have the L2 norm so that we can use the inner product between two embedding vectors as a measure of cosine similarity.\n",
    "This further allows us to perform the index search using FAISS, which is optimized for such operations.\n",
    "'''\n",
    "if E.dtype != np.float32 or not E.flags['C_CONTIGUOUS']:\n",
    "    E = np.ascontiguousarray(E, dtype=np.float32)\n",
    "\n",
    "d = faiss.normalize_L2(E)\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(E)\n",
    "D, I = index.search(E, k=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ed9f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 3: \n",
    "We create a graph where each node is a task statement, and we add an edge between two nodes if their cosine similarity is \n",
    "above a certain threshold (0.97 in this case). This allows us to identify clusters of similar task statements.\n",
    "'''\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(data)))\n",
    "threshold = 0.97\n",
    "for i, neighbors in enumerate(I):\n",
    "    for j, score in zip(neighbors, D[i]):\n",
    "        if i < j and score >= threshold:\n",
    "            G.add_edge(i, j)\n",
    "clusters = list(nx.connected_components(G))\n",
    "canon = {}\n",
    "for cid, comp in enumerate(clusters, 1):\n",
    "    for idx in comp:\n",
    "        canon[idx] = f\"C{cid:05d}\"\n",
    "data['canon_id'] = data.index.map(canon)\n",
    "data.to_csv('tasks_with_canon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bafceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by canon id and show all unique task statements in each cluster, get counts\n",
    "grouped = data.groupby('canon_id')['Task'].apply(lambda x: ', '.join(x.unique())).reset_index()\n",
    "grouped['count'] = grouped['Task'].apply(lambda x: len(x.split(', ')))\n",
    "grouped[grouped['count'] > 1].to_csv('task_clusters_geq_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a438bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canon_id</th>\n",
       "      <th>Task</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00002</td>\n",
       "      <td>Administers, interprets, and explains policies...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00003</td>\n",
       "      <td>Develops, plans, organizes, and administers po...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00004</td>\n",
       "      <td>Directs and coordinates activities of workers ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C00007</td>\n",
       "      <td>Reviews and analyzes legislation, laws, and pu...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C00008</td>\n",
       "      <td>Develops, directs, and coordinates testing, hi...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22917</th>\n",
       "      <td>C22918</td>\n",
       "      <td>Provide nursing, psychiatric, or personal care...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22918</th>\n",
       "      <td>C22919</td>\n",
       "      <td>Collaborate with or assist doctors, psychologi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22919</th>\n",
       "      <td>C22920</td>\n",
       "      <td>Provide patients with cognitive, intellectual,...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22922</th>\n",
       "      <td>C22923</td>\n",
       "      <td>Provide care for children with physical, devel...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22924</th>\n",
       "      <td>C22925</td>\n",
       "      <td>Fit garments on clients, altering as needed.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15465 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      canon_id                                               Task  count\n",
       "1       C00002  Administers, interprets, and explains policies...      6\n",
       "2       C00003  Develops, plans, organizes, and administers po...      4\n",
       "3       C00004  Directs and coordinates activities of workers ...      3\n",
       "6       C00007  Reviews and analyzes legislation, laws, and pu...     11\n",
       "7       C00008  Develops, directs, and coordinates testing, hi...      6\n",
       "...        ...                                                ...    ...\n",
       "22917   C22918  Provide nursing, psychiatric, or personal care...      5\n",
       "22918   C22919  Collaborate with or assist doctors, psychologi...      7\n",
       "22919   C22920  Provide patients with cognitive, intellectual,...      6\n",
       "22922   C22923  Provide care for children with physical, devel...      3\n",
       "22924   C22925       Fit garments on clients, altering as needed.      2\n",
       "\n",
       "[15465 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e371b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailabor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
